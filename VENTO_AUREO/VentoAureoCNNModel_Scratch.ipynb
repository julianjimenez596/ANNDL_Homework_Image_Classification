{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_6NekpaDZzc"
   },
   "source": [
    "# Artificial Neural Networks And Deep Learning \n",
    "# Homework 1: Image Classification\n",
    "# CNN Model from Scratch\n",
    "Students: \n",
    "\n",
    "*   Julián Jiménez: 10657117\n",
    "*   Samuel Polo: 10670388\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6BjhSLrDTXs"
   },
   "source": [
    "# **Our approach of the Task:**\n",
    "# Initial approach\n",
    "\n",
    "We started the project with the basic CNN Model that we created in the Laboratory session of the class in order to do some basic tests. Without making any changes, our first score was 0.49800. \n",
    "\n",
    "After this, we began to do some minor changes in order to increase our perfomance, and our second test made was making data augmentation, in which we increased our score up to 0.61600.\n",
    "\n",
    "Then we settled the following list of tests:\n",
    "\n",
    "\n",
    "\n",
    "*   The same model with L2 regularization (score: 0.57399)\n",
    "*   Modification of the split value to 0.25 and L1_L2 regularization (score: 0.55600)\n",
    "\n",
    "*   Additions of neurons to the fully connected layer (score: 0.51800)\n",
    "*   Additions of neurons and regularization (score: 0.53200)\n",
    "\n",
    "*   Modification of Elu as activation function with the previous model (score: 0.53400)\n",
    "*   The previous model with a modified validation set separation (score: 0.57799)\n",
    "\n",
    "*   The previous model with a greater number of filters for the CNN (score: 0.51800)\n",
    "\n",
    "Surprisingly, after all our tests we couldn't improve the result from our second test, so we decided to approach the problem with the Transfer Learning method.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "E7dguOIHdgQy",
    "outputId": "85942223-af5b-4e76-91dd-809b44bb2ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGDkuBSvmK3e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5h2al_SumOry",
    "outputId": "e2200b03-788d-46d1-b853-3468aa473fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for random operations. \n",
    "# This lets our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)  \n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49Kq3pV57FuG"
   },
   "outputs": [],
   "source": [
    "#Set the directories for all, training and validation images.\n",
    "dataset_dir = os.path.join(cwd, 'Classification_Dataset')\n",
    "training_dir = os.path.join(dataset_dir,'training')\n",
    "valid_dir = os.path.join(dataset_dir,'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gHDhhGeiG-mE"
   },
   "source": [
    "## Validation Directory Creation:\n",
    "\n",
    "This code is used to create the validation directory and fill it up with images from all the classes.\n",
    "The validation set is created with stratified sampling since all classes are not represented equally in the given dataset. In this way from each class we select the same proportion of images. This is done to reduce sampling error and allow all classes to be represented in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kD6PnFA6HDxf"
   },
   "outputs": [],
   "source": [
    "#Creates validation directory if it does not exist already.\n",
    "_ = os.makedirs(valid_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mmbj0L_V-fy5"
   },
   "outputs": [],
   "source": [
    "#Proportion for Hold-out set validation.\n",
    "train_valid_split = 0.2\n",
    "\n",
    "#Fills the validtaion set \n",
    "subfolders = [f.path for f in os.scandir(training_dir) if f.is_dir() ] \n",
    "\n",
    "for subfold in subfolders:\n",
    "  head_tail = os.path.split(subfold)\n",
    "  _ = os.makedirs(os.path.join(valid_dir,head_tail[1]), exist_ok=True)\n",
    "  list_files = [name for name in os.scandir(subfold) if os.path.isfile(name)]\n",
    "  num_files = len(list_files)\n",
    "  for num in np.random.choice(num_files, int(num_files*0.2) ,replace=False):\n",
    "    file_name = os.path.split(list_files[num])\n",
    "    os.rename(list_files[num],os.path.join(valid_dir,head_tail[1],file_name[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pvX6qgAjHSPf"
   },
   "source": [
    "# Training and Validation Generator and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJluarmumlei"
   },
   "outputs": [],
   "source": [
    "# ImageDataGenerator\n",
    "# ------------------\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "apply_data_augmentation = True\n",
    "\n",
    "# Create training ImageDataGenerator object\n",
    "if apply_data_augmentation:\n",
    "    train_data_gen = ImageDataGenerator(rotation_range=20,\n",
    "                                        width_shift_range=0.2,\n",
    "                                        height_shift_range=0.2,\n",
    "                                        zoom_range=0.3,\n",
    "                                        horizontal_flip=True,\n",
    "                                        vertical_flip=True,\n",
    "                                        fill_mode='constant',\n",
    "                                        cval=0,\n",
    "                                        rescale=1./255)\n",
    "else:\n",
    "    train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "valid_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZaNqVt2m04o"
   },
   "outputs": [],
   "source": [
    "#Batch Size\n",
    "bs = 64\n",
    "# img shape\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "num_classes = 20\n",
    "\n",
    "classes = ['owl',              # 0\n",
    "           'galaxy',           # 1\n",
    "           'lightning',        # 2\n",
    "           'wine-bottle',      # 3\n",
    "           't-shirt',          # 4\n",
    "           'waterfall',        # 5\n",
    "           'sword',            # 6\n",
    "           'school-bus',       # 7\n",
    "           'calculator',       # 8\n",
    "           'sheet-music',      # 9\n",
    "           'airplanes',        # 10\n",
    "           'lightbulb',        # 11\n",
    "           'skyscraper',       # 12\n",
    "           'mountain-bike',     # 13\n",
    "           'fireworks',        # 14\n",
    "           'computer-monitor', # 15\n",
    "           'bear',             # 16\n",
    "           'grand-piano',      # 17\n",
    "           'kangaroo',         # 18\n",
    "           'laptop']           # 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "armPwaB3mTva",
    "outputId": "e86eaae0-d2f2-4436-ad6c-3d0443307ec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1247 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "# Training Generator\n",
    "train_gen = train_data_gen.flow_from_directory(training_dir,\n",
    "                                               batch_size=bs,\n",
    "                                               classes=classes,\n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=True,\n",
    "                                               target_size=(img_h,img_w),\n",
    "                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "NdxNV734L_Ea",
    "outputId": "1d1b27a0-2278-424a-ac5b-f657ca0268a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 307 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "#Validation Generator\n",
    "valid_gen = train_data_gen.flow_from_directory(valid_dir,\n",
    "                                               batch_size=bs,\n",
    "                                               classes=classes,\n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=False,\n",
    "                                               target_size=(img_h,img_w),\n",
    "                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJY0ROzWsbiT"
   },
   "outputs": [],
   "source": [
    "#Create Dataset objects\n",
    "#Training\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
    "train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3rwIrFocsgKq"
   },
   "outputs": [],
   "source": [
    "#Validation\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen,\n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRgV1ABnIXpl"
   },
   "source": [
    "# **Creation of the CNN Model**\n",
    "## **Keras Model Subclass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8uXMOYdYsjej"
   },
   "outputs": [],
   "source": [
    "# Keras Model subclassing \n",
    "\n",
    "# Create convolutional block\n",
    "class ConvBlock(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv2d = tf.keras.layers.Conv2D(filters=num_filters,\n",
    "                                             kernel_size=(3, 3),\n",
    "                                             strides=(1, 1), \n",
    "                                             padding='same')\n",
    "        self.activation = tf.keras.layers.ReLU() \n",
    "        self.pooling = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv2d(inputs)\n",
    "        x = self.activation(x)\n",
    "        x = self.pooling(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbLhK3U_spvU"
   },
   "outputs": [],
   "source": [
    "# Create Model\n",
    "# ------------\n",
    "\n",
    "depth = 5\n",
    "start_f = 64\n",
    "num_classes = 20\n",
    "\n",
    "class CNNClassifier(tf.keras.Model):\n",
    "    def __init__(self, depth, start_f, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = tf.keras.Sequential()\n",
    "    \n",
    "        for i in range(depth):\n",
    "            self.feature_extractor.add(ConvBlock(num_filters=start_f))\n",
    "            start_f *= 2\n",
    "            \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.classifier = tf.keras.Sequential()\n",
    "        self.classifier.add(tf.keras.layers.Dense(units=4096, activation='relu'))\n",
    "        self.classifier.add(tf.keras.layers.Dense(units=4096, activation='relu'\n",
    "                                                  ))\n",
    "        self.classifier.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.feature_extractor(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "# Create Model instance\n",
    "model = CNNClassifier(depth=depth,\n",
    "                      start_f=start_f,\n",
    "                      num_classes=num_classes)\n",
    "# Build Model (Required)\n",
    "model.build(input_shape=(None, img_h, img_w, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AO6ttFCANyH1"
   },
   "source": [
    "## **Definition of Parameters for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eG9e0LMYswZZ"
   },
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "colab_type": "code",
    "id": "JHITVyjts2pS",
    "outputId": "e7907051-27bd-4558-9ce0-55ec659c2a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_block_10 (ConvBlock)    multiple                  1792      \n",
      "_________________________________________________________________\n",
      "conv_block_11 (ConvBlock)    multiple                  73856     \n",
      "_________________________________________________________________\n",
      "conv_block_12 (ConvBlock)    multiple                  295168    \n",
      "_________________________________________________________________\n",
      "conv_block_13 (ConvBlock)    multiple                  1180160   \n",
      "_________________________________________________________________\n",
      "conv_block_14 (ConvBlock)    multiple                  4719616   \n",
      "=================================================================\n",
      "Total params: 6,270,592\n",
      "Trainable params: 6,270,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Visualize created model as a table\n",
    "model.feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rIGUus2FM7bi"
   },
   "source": [
    "# **Training Procedure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NzAwNA8arZL8",
    "outputId": "f59b30c9-fe42-4aed-d83e-479b9b25388f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 20 steps, validate for 5 steps\n",
      "Epoch 1/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 4.5232 - accuracy: 0.0569 - val_loss: 2.9659 - val_accuracy: 0.0651\n",
      "Epoch 2/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 2.9738 - accuracy: 0.0666 - val_loss: 2.9271 - val_accuracy: 0.1205\n",
      "Epoch 3/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 2.8836 - accuracy: 0.0962 - val_loss: 2.8022 - val_accuracy: 0.1173\n",
      "Epoch 4/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 2.7775 - accuracy: 0.0954 - val_loss: 2.8520 - val_accuracy: 0.1173\n",
      "Epoch 5/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 2.7861 - accuracy: 0.1115 - val_loss: 2.7165 - val_accuracy: 0.1336\n",
      "Epoch 6/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.7222 - accuracy: 0.1195 - val_loss: 2.7806 - val_accuracy: 0.0945\n",
      "Epoch 7/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.6907 - accuracy: 0.1235 - val_loss: 2.6481 - val_accuracy: 0.1401\n",
      "Epoch 8/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.6648 - accuracy: 0.1299 - val_loss: 2.6433 - val_accuracy: 0.1596\n",
      "Epoch 9/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.6007 - accuracy: 0.1395 - val_loss: 2.6266 - val_accuracy: 0.1564\n",
      "Epoch 10/200\n",
      "20/20 [==============================] - 55s 3s/step - loss: 2.5510 - accuracy: 0.1812 - val_loss: 2.6810 - val_accuracy: 0.1368\n",
      "Epoch 11/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 2.5162 - accuracy: 0.1788 - val_loss: 2.5028 - val_accuracy: 0.1824\n",
      "Epoch 12/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 2.4195 - accuracy: 0.2101 - val_loss: 2.5061 - val_accuracy: 0.1954\n",
      "Epoch 13/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 2.3280 - accuracy: 0.2277 - val_loss: 2.4627 - val_accuracy: 0.2182\n",
      "Epoch 14/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 2.3339 - accuracy: 0.2350 - val_loss: 2.4749 - val_accuracy: 0.2182\n",
      "Epoch 15/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.3842 - accuracy: 0.2310 - val_loss: 2.4335 - val_accuracy: 0.2345\n",
      "Epoch 16/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.2164 - accuracy: 0.2863 - val_loss: 2.2999 - val_accuracy: 0.2964\n",
      "Epoch 17/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.1949 - accuracy: 0.2791 - val_loss: 2.4927 - val_accuracy: 0.2150\n",
      "Epoch 18/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.2395 - accuracy: 0.2574 - val_loss: 2.3203 - val_accuracy: 0.2932\n",
      "Epoch 19/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.1068 - accuracy: 0.3176 - val_loss: 2.3136 - val_accuracy: 0.2736\n",
      "Epoch 20/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.1076 - accuracy: 0.3063 - val_loss: 2.2681 - val_accuracy: 0.2704\n",
      "Epoch 21/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.0644 - accuracy: 0.3376 - val_loss: 2.3058 - val_accuracy: 0.3192\n",
      "Epoch 22/200\n",
      "20/20 [==============================] - 58s 3s/step - loss: 2.0384 - accuracy: 0.3545 - val_loss: 2.1834 - val_accuracy: 0.3257\n",
      "Epoch 23/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 1.9542 - accuracy: 0.3480 - val_loss: 2.2752 - val_accuracy: 0.2866\n",
      "Epoch 24/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 1.9695 - accuracy: 0.3304 - val_loss: 2.2467 - val_accuracy: 0.3160\n",
      "Epoch 25/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 1.8799 - accuracy: 0.3801 - val_loss: 2.0801 - val_accuracy: 0.3583\n",
      "Epoch 26/200\n",
      "20/20 [==============================] - 55s 3s/step - loss: 1.8311 - accuracy: 0.4082 - val_loss: 2.1436 - val_accuracy: 0.3355\n",
      "Epoch 27/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.8259 - accuracy: 0.3978 - val_loss: 2.0782 - val_accuracy: 0.3681\n",
      "Epoch 28/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.7722 - accuracy: 0.4122 - val_loss: 2.1208 - val_accuracy: 0.3550\n",
      "Epoch 29/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.7283 - accuracy: 0.4186 - val_loss: 1.9987 - val_accuracy: 0.3876\n",
      "Epoch 30/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.7142 - accuracy: 0.4395 - val_loss: 2.2783 - val_accuracy: 0.3550\n",
      "Epoch 31/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.7261 - accuracy: 0.4290 - val_loss: 2.1976 - val_accuracy: 0.3388\n",
      "Epoch 32/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 1.6787 - accuracy: 0.4274 - val_loss: 2.1147 - val_accuracy: 0.3518\n",
      "Epoch 33/200\n",
      "20/20 [==============================] - 57s 3s/step - loss: 1.6577 - accuracy: 0.4507 - val_loss: 2.1225 - val_accuracy: 0.3844\n",
      "Epoch 34/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.6822 - accuracy: 0.4507 - val_loss: 2.1576 - val_accuracy: 0.3550\n",
      "Epoch 35/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.6231 - accuracy: 0.4683 - val_loss: 1.9927 - val_accuracy: 0.3909\n",
      "Epoch 36/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.5146 - accuracy: 0.4972 - val_loss: 1.9045 - val_accuracy: 0.4365\n",
      "Epoch 37/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.5668 - accuracy: 0.4771 - val_loss: 1.8450 - val_accuracy: 0.4495\n",
      "Epoch 38/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.4942 - accuracy: 0.4932 - val_loss: 1.9042 - val_accuracy: 0.4137\n",
      "Epoch 39/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.4763 - accuracy: 0.5140 - val_loss: 1.9367 - val_accuracy: 0.4104\n",
      "Epoch 40/200\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.4552 - accuracy: 0.5060 - val_loss: 2.0249 - val_accuracy: 0.4267\n",
      "Epoch 41/200\n",
      "19/20 [===========================>..] - ETA: 1s - loss: 1.4557 - accuracy: 0.5216"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "exps_dir = os.path.join(cwd, 'classification_experiments')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "model_name = 'CNN'\n",
    "\n",
    "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    \n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoint\n",
    "# ----------------\n",
    "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                   save_weights_only=True)  # False to save the model directly\n",
    "callbacks.append(ckpt_callback)\n",
    "\n",
    "# Early Stopping\n",
    "# Restores the best weights found during the training\n",
    "# --------------\n",
    "early_stop = True\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "\n",
    "model.fit(x=train_dataset,\n",
    "          epochs=200,  #### set repeat in training dataset\n",
    "          steps_per_epoch=len(train_gen),\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=len(valid_gen), \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Oi_cnjvNQy5"
   },
   "source": [
    "# **Output of the Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVq1pzbMNWv6"
   },
   "source": [
    "# **CSV Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im8hWuFaraQc"
   },
   "outputs": [],
   "source": [
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfS3XZ7MNbzH"
   },
   "source": [
    "## **Test set label assignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGD_H9rEurc7"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from skimage import transform\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "image_filenames = next(os.walk(test_dir))[2]\n",
    "\n",
    "results = {}\n",
    "for image_name in image_filenames:\n",
    "  #print(image_name)\n",
    "  image_dir = os.path.join(test_dir,image_name)\n",
    "  img = Image.open(image_dir).convert('RGB')\n",
    "  img_array = np.array(img)\n",
    "  img_array = np.array(img_array).astype('float32')/255.\n",
    "  img_array = transform.resize(img_array, (256, 256, 3))\n",
    "  img_array = np.expand_dims(img_array, 0)\n",
    "  pred = model.predict(img_array)\n",
    "  prediction = np.argmax(model.predict(img_array))   # predicted class\n",
    "  results[image_name] = prediction\n",
    "\n",
    "create_csv(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VentoAureoCNNModel_Scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
